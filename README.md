# transformer-head-attention-pruning
Analyzing and pruning attention heads in T5 transformers using gradient-based importance scores, with evaluation on summarization and translation tasks from scratch.
